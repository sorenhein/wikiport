Some deals should be split into several rounds (A, B, ...)

Compare dates to Oliver's

Can Klaus Feix be an owner without having an account?

Use field Reasons to decline better

New segmentation of MIG deal categories



In parse.pl, perhaps recurse within own tree of a deal.

Subtract all the good ones from JK list.

Play around with getting the HTML and preserving links (rebasing them).

In the end, check that "all deal pages" are referenced in Affinity.


Move portfolio out of deal list into portfolio.txt

Are the many pages that are not linked strictly hierarchically?

Do use the permission knowledge (PipelineGroup as well?)

oddlines.txt (no ___) and badlinks.txt (ditto) should be read in
nested.pl and ignored when they show up.  Maybe they should have 
the ___ back.  Maybe they should be concatenated in deadlinks.txt.

Add a MIG mail user.  Use this as the source.

Check which ones of koschcand.txt are real deals.  Perhaps use their
groups as well.

Among JK deal candidates, 3 weren't read as they differ only by case
from existing ones (and Cygwin, unlike Wiki, doesn't differentiate).
- Diacdem (DiaCdem exists)
- Leadmacher (LEADMACHER exists)
- Nanotemper (NanoTemper exists)
- Tigo (TIGO exists)

nout.txt has the 2nd level of linked pages from the Affinity list.
Download this from Python, into nested directory to begin with.
Careful not to overwrite miss.txt from the Jürgen candidates
currently running.


2020-04-19, 13:10
-----------------
4883 entries in Deal List
4840 entries in export
  43 entries without info.wiki.ag links (probably all mine)

4349 unique links, so
 491 duplicates (Prüfung_...)

4349 files in deals/found


Order
-----
* Export from Affinity
* perl getfile.pl 2020-04-19-07-40/list.csv > 2020-04-19-07-40/list.txt
* cp !$ ../../scripts
* sort list.txt | uniq > a.txt; mv a.txt list.txt
* run python3 fetch.py list.txt
* In ../data/deals/found directory: wc | ls.  Maybe also check content.


Hierarchy
---------
Hierarchical pages within known deals:
* perl nested.pl > links.txt
* python3 fetch.py links.txt
  - Fills out notfound/miss.txt
  - Fills out nests/ with downloads containing ___
* Add the dead ones to deadlinks.txt, used by parse.pl          <------
* Move into deals/ (gingerly).
* Repeat a couple of times until no more new, nested downloads.


oddlinks and badlinks are links referenced on Wiki pages in the 
Affinity list.  The odd ones are in the Wiki list but can't be read.
The bad ones are not in the Wiki list at all.
